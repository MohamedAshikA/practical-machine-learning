Practical machine learning - Course project
FLat
2020-12-06
Introduction
In the scenario at hand the data set is on personal activity including numerous measurements from body held accelerometers. The goal is to model the manner, or quality of activities performed based on this data.

Analysis
Configurations
We will use the caret package for modeling.

library(data.table)
library(caret)
## Loading required package: lattice
## Loading required package: ggplot2
set.seed(1)
Data Import
training_data_raw <- fread("Data/pml-training.csv", header = TRUE)
testing_data_raw <- fread("Data/pml-testing.csv", header = TRUE)
Data cleaning and preparations
Columns that are irrelevant to the task such as test subject, or timestamps are removed, as well as columns with missing values, or non-numeric data. The target variable “classe” is kept and transformed into a factorial variable.
The training data set is split into a training and a validation set for crossvalidation using a common ratio of 70 % to 30 %.

# Remove irrelevant columns
training_data_cleaned <- training_data_raw[, - c(1:7)]

# Remove columns with NAs
training_data_cleaned <- training_data_cleaned[, colSums(is.na(training_data_cleaned)) == 0, with = FALSE]
training_data_cleaned <- training_data_cleaned[, which(sapply(training_data_cleaned, is.numeric) == TRUE | names(training_data_cleaned) == "classe"), with = FALSE]
training_data_cleaned[, classe := as.factor(training_data_cleaned$classe)]
training_near_zero <- nearZeroVar(training_data_cleaned)

if (length(training_near_zero) > 0) {
      training_data_cleaned <- training_data_cleaned[, - c(training_near_zero)]
}

# Split data set into training and validation data
split_obs <- createDataPartition(training_data_cleaned$classe, p = 0.70, list = FALSE)

training_data <- training_data_cleaned[split_obs, ]
validation_data <- training_data_cleaned[- split_obs, ]
Modeling
Gradient boosting machine and random forest modeling is used for modeling. Accuracy, out of sample errors and predictions of both approaches are compared. Equal predictions would underline the validity. In case predictions are different, the approach with higher accuracy will be chosen.

GBM model
# GBM modeling
control_gbm <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
gbm  <- train(classe ~ ., data = training_data, method = "gbm", trControl = control_gbm, verbose = FALSE)

# GBM validation
predict_gbm <- predict(gbm, newdata = validation_data)
cm_gbm <- confusionMatrix(predict_gbm, validation_data$classe)
cm_gbm
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1634   31    0    3    1
##          B   23 1076   26    2   14
##          C   10   31  990   26    7
##          D    5    0    8  928   12
##          E    2    1    2    5 1048
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9645          
##                  95% CI : (0.9594, 0.9691)
##     No Information Rate : 0.2845          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.9551          
##                                           
##  Mcnemar's Test P-Value : 1.169e-05       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9761   0.9447   0.9649   0.9627   0.9686
## Specificity            0.9917   0.9863   0.9848   0.9949   0.9979
## Pos Pred Value         0.9790   0.9430   0.9305   0.9738   0.9905
## Neg Pred Value         0.9905   0.9867   0.9925   0.9927   0.9930
## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
## Detection Rate         0.2777   0.1828   0.1682   0.1577   0.1781
## Detection Prevalence   0.2836   0.1939   0.1808   0.1619   0.1798
## Balanced Accuracy      0.9839   0.9655   0.9748   0.9788   0.9832
Random forest model
# RF modeling
control_rf <- trainControl(method = "cv", 5)
rf <- train(classe ~ ., data = training_data, method = "rf", trControl = control_rf, ntree = 250)

# RF validation
predict_rf <- predict(rf, validation_data)
cm_rf <- confusionMatrix(validation_data$classe, predict_rf)
cm_rf
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1670    2    2    0    0
##          B    7 1128    3    1    0
##          C    0    3 1020    3    0
##          D    0    0    4  958    2
##          E    0    1    1    2 1078
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9947          
##                  95% CI : (0.9925, 0.9964)
##     No Information Rate : 0.285           
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.9933          
##                                           
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9958   0.9947   0.9903   0.9938   0.9981
## Specificity            0.9990   0.9977   0.9988   0.9988   0.9992
## Pos Pred Value         0.9976   0.9903   0.9942   0.9938   0.9963
## Neg Pred Value         0.9983   0.9987   0.9979   0.9988   0.9996
## Prevalence             0.2850   0.1927   0.1750   0.1638   0.1835
## Detection Rate         0.2838   0.1917   0.1733   0.1628   0.1832
## Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
## Balanced Accuracy      0.9974   0.9962   0.9945   0.9963   0.9987
Comparing accuracy and out of sample errors
The accuracy of the BGM model is 0.9645 resulting in an out of sample error of 0.0355.
The accuracy of the random forest model is 0.9947 resulting in an out of sample error of 0.0053. Leaving the random forest model the more accurate one.

Comparing predictions on the testing data
# Predicting the 20 observations test set
result_gbm <- predict(gbm, testing_data_raw)
result_rf <- predict(rf, testing_data_raw)

result_gbm
##  [1] B A B A A E D D A A B C B A E E A B B B
## Levels: A B C D E
result_rf
##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E
The sequence above shows the evaluation of the 20 instances of exercises in the testing data set. The prediction sequence of both models is identical supporting 
